---
layout: single
title: "Opinion spam detection - Literature review"
modified: 2014-09-07 20:31:35 +0200
tags: [opinion spam, fake reviews]
---

<p>I have decided to make a sort of a series about several findings in my <a href="http://www.vladsandulescu.com/opinion-spam-detection-through-semantic-similarity/">thesis about opinion spam detection</a>. This is the pilot episode, a literature review of the most significant research papers on opinion spam until now.</p>

<p>OK, here goes.
The opinion spam problem was first formulated by Jindal and Liu in the context of product reviews, <a href="#Jindal2008">(Jindal &amp; Liu, 2008)</a>. By analyzing several million reviews from the popular Amazon.com, they showed how widespread the problem of fake reviews was. The existing detection methods can be split in the context of machine learning into supervised and unsupervised approaches. Second, they can be split into three categories by their features: behavioral, linguistic or those using a combination of these two.
They categorized spam reviews into three categories: non-reviews, brand-only reviews and untruthful reviews. The authors ran a logistic regression classifier on a model trained on duplicate or near-duplicate reviews as positive training data, i.e. fake reviews, and the rest of the reviews they used as truthful reviews. They combined reviewer behavioral features with textual features and they aimed to demonstrate that the model could be generalized to detect non-duplicate review spam. This was the first documented research on the problem of opinion spam and thus did not benefit from existing training databases. The authors had to build their own dataset, and the simplest approach was to use near-duplicate reviews as examples of deceptive reviews. Although this initial model showed good results, it is still an early investigation into this problem.</p>

<p><a href="#Lim2010">(Lim, Nguyen, Jindal, Liu, &amp; Lauw, 2010)</a> is also an early work on detecting review spammers which proposed scoring techniques for the spamicity degree of each reviewer. The authors tested their model on Amazon reviews, which were initially taken through several data preprocessing steps. In this stage, they decided to only keep reviews from highly active users - users that had written at least 3 reviews. The detection methods are based on several predefined abnormalities indicators, such as general rating deviation, early deviation - i.e. how soon after a product appears on the website does a suspicious user post a review about it or very high/low ratings clusters. The features weights were linearly combined towards a spamicity formula and computed empirically in order to maximize the value of the normalized discounted cumulative gain measure. The measure showed how well a particular ranking improves on the overall goal. The training data was constructed as mentioned earlier from Amazon reviews, which were manually labeled by human evaluators. Although an agreement measure is used to compute the inter-evaluator agreement percentage, so that a review is considered fake if all of the human evaluators agree, this method of manually labeling deceptive reviews has been proven to lead to low accuracy when testing on real-life fake review data. 
First, <a href="#Ott2011">(Ott, Choi, Cardie, &amp; Hancock, 2011)</a> demonstrated that it is impossible for humans to detect fake reviews simply by reading the text. Second, <a href="#Mukherjee2012">(Mukherjee, Liu, &amp; Glance, 2012)</a> proved that not even fake reviews produced through crowdsourcing methods are valid training data because the models do not generalize well on real-life test data.</p>

<p><a href="#Wang2012">(Wang, Xie, Liu, &amp; Yu, 2012)</a> considered the triangular relationship among stores, reviewers and their reviews. This was the first study to capture such relationships between these concepts and study their implications. They introduced 3 measures meant to do this: the stores reliability, the trustworthiness of the reviewers and the honesty of the reviews. Each concept depends on the other two, in a circular way, i.e. a store is more reliable when it contains honest reviews written by trustworthy reviewers and so on for the other two concepts. They proposed a heterogeneous graph based model, called the review graph, with 3 types of nodes, each type of node being characterized by a spamicity score inferred using the other 2 types. In this way, they aimed to capture much more information about stores, reviews and reviewers than just focus on behavioral reviewer centric features. This is also the first study on store reviews, which are different than product reviews. The authors argue that when looking at product reviews, while it may be suspicious to have multiple reviews from the same person for similar products, it is ok for the same person to buy multiple similar products from the same store and write a review every time about the experience. In almost all fake product reviews, studies which use the cosine similarity as a measure of review content alikeness, a high value is considered as a clear signal of cheating, since the spammers do not spend much time writing new reviews all the time, but reuse the exact same words. However, when considering store reviews, it is possible for the same user to make valid purchases from similar stores, thus reusing the content of his older reviews and not writing completely different reviews all the time. <a href="#Wang2012">(Wang, Xie, Liu, &amp; Yu, 2012)</a> used an iterative algorithm to rank the stores, reviewers and reviews respectively, claiming that top rankers in each of the 3 categories are suspicious. They evaluated their top 10 top and bottom ranked spammer reviewers results using human evaluators and computed the inter-evaluator agreement. The evaluation of the resulted store reliability score, again for the top 10 top and bottom ranked stores was done by comparison with store data from Better Business Bureaus, a corporation that keeps track businesses reliability and possible consumer scams. </p>

<p><a href="#Xie2012">(Xie, Wang, Lin, &amp; Yu, 2012)</a> observed that the vast majority of reviewers (more than 90% in their study or resellerratings.com reviews up to 2010) only wrote one review, so they have focused their research on this type of reviewers. They also claim, similarly to <a href="#Feng2012">(Feng, Xing, Gogar, &amp; Choi, 2012)</a>, that a flow of fake reviews coming from a hired spammer distorts the usual distribution of ratings for the product, leaving distributional traces behind. Xie et al. observed the normal flow of reviews is not correlated with the given ratings over time. Fake reviews come in bursts of either very high ratings, i.e. 5-stars, or very low ratings, i.e. 1-star, so the authors aim to detect time windows in which these abnormally correlated patterns appear. They considered the number of reviews, average ratings and the ratio of singleton reviews which stick out when looking over different time windows. The paper makes important contributions to opinion spam detection by being the first study to date to formulate the singleton spam review problem. Previous works have disregarded this aspect completely by purging singleton reviews from their training datasets and focusing more on tracking the activity of reviewers as they make multiple reviews. It is of course reasonable to claim that the more information is saved about a user and the more data points about a user’s activity exist, the easier it is to profile that user and assert with greater accuracy whether he is a spammer or not. Still, it is simply not negligible that a large percentage of users on review platforms write only one review.  </p>

<p><a href="#Feng2012">(Feng, Xing, Gogar, &amp; Choi, 2012)</a> published the first study to tackle the opinion spam as a distributional anomaly problem, considering crawled data from Amazon and TripAdvisor. They claim product reviews are characterized by natural distributions which are distorted by hired spammers when writing fake reviews. Their contribution consists of first introducing the notion of natural distribution of opinions and second of conducting a range of experiments that finds a connection between distributional anomalies and the time windows when deceptive reviews were written. For the purpose of evaluation they used a gold standard dataset containing 400  known deceptive reviews written by hired people, created by <a href="#Ott2011">(Ott, Choi, Cardie, &amp; Hancock, 2011)</a>. Their proposed method achieves a maximum accuracy of only 72.5% on the test dataset and thus is suitable as a technique to pinpoint suspicious activity within a time window and draw attention on suspicious products or brands. This technique does not solely represent however a complete solution where individual reviews can be deemed as fake or truthful, but simply brings to the foreground delimited short time windows where methods from other studies can be applied to detect spammers.</p>

<p><a href="#Li2011">(Li, Huang, Yang, &amp; Zhu, 2011)</a> have used supervised learning and manually labeled reviews crawled from Epinions to detect product review spam. They also added to the model the helpfulness scores and comments the users associated with each review. Due to the dataset size of about 60K reviews and the fact that manual labeling was required, an important assumption was made - reviews that receive fewer helpful votes from people are more suspicious. Based on this assumption, they have filtered out review data accordingly, e.g. only considering reviews which have at least 5 helpfulness votes or comments.
They achieved a 0.58 F-Score result using their supervised method model, which outperformed the heuristic methods used at that time to detect review spam. However, this result is very low when compared with that of more recent review spam detection models. The main reason for this has been the training of the model on manually labeled fake reviews data, as well as the initial data pre-processing step where reviews were selected based on their helpfulness votes. <a href="#Mukherjee2013">(Mukherjee et al., 2013)</a> also makes the  assumption that deceptive reviews get less votes. But their model evaluation later showed that helpfulness votes not only perform poorly but they may also be abused - groups of spammers working together to promote certain products may give many votes to each others reviews. The same conclusion has been also expressed by <a href="#Lim2010">(Lim, Nguyen, Jindal, Liu, &amp; Lauw, 2010)</a>.</p>

<p><a href="#Ott2011">(Ott, Choi, Cardie, &amp; Hancock, 2011)</a> produced the first dataset of gold-standard deceptive opinion spam, employing crowdsourcing through the Amazon Mechanical Turk. They demonstrated that humans cannot distinguish fake reviews by simply reading the text, the results of these experiments showing an  at-chance probability. The authors found that although part-of-speech n-gram features give a fairly good prediction on whether an individual review is fake, the classifier actually performed slightly better when psycholinguistic features were added to the model. The expectation was also that truthful reviews resemble more of an informative writing style, while deceptive reviews are more similar in genre to imaginative writing. The authors coupled the part-of-speech tags in the review text which had the highest frequency distribution with the results obtained from a text analysis tool previously used to analyze deception. Testing their classifier against the gold-standard dataset, they revealed clue words deemed as signs of deceptive writing. However, this can be seen as overly simplistic, as some of these words, which according to the results have a higher probability to appear in a fake review, such as “vacation” or “family”, may as well appear in truthful reviews. The authors finally concluded that the domain context has an important role in the feature selection process. Simply put, the imagination of spammers is limited - e.g. in the case of hotel reviews, they tend to not be able to give spatial details regarding their stay. While the classifier scored good results on the gold-standard dataset, once the spammers learn about them, they could simply avoid using the particular clue words, thus lowering the classifier accuracy when applied to real-life data on the long term. </p>

<p><a href="#Mukherjee2012">(Mukherjee, Liu, &amp; Glance, 2012)</a> were the first to try to solve the problem of opinion spam resulted from a group collaboration between multiple spammers. The method they proposed first extracts candidate groups of users using a frequent itemset mining technique. For each group, several individual and group behavioral indicators are computed, e.g. the time differences between group members when posting, the rating deviation between group members compared with the rest of the product reviewers, the number of products the group members worked together on, or review content similarities. The authors also built a dataset of fake reviews, with the help of human judges which manually labeled a number of reviews. They experimented both with learning to rank methods, i.e. ranking of groups based on their spamicity score and with classification using SVM and logistic regression, using the labeled review data for training. The algorithm, called GSRank considerably outperformed existing methods by achieving an area under the curve result (AUC) of 95%. This score makes it a very strong candidate for production environments where the community of users is very active and each user writes more than one review. However, not many users write a lot of reviews, there exists a relatively small percentage of “elite” contributing users. So this method would best be coupled with a method for detecting singleton reviewers, such as the method from <a href="#Xie2012">(Xie, Wang, Lin, &amp; Yu, 2012)</a>. </p>

<p><a href="#Mukherjee2013b">(Mukherjee, Venkataraman, Liu, &amp; Glance, 2013)</a> have questioned the validity of previous research results based on supervised learning techniques trained on Amazon Mechanical Turk (AMT) generated fake reviews. They tested the method of <a href="#Ott2011">(Ott, Choi, Cardie, &amp; Hancock, 2011)</a> on known fake reviews from Yelp. The assumption was that the company had perfected its detection algorithm for the past decade and so its results should be trustworthy. Surprisingly, unlike <a href="#Ott2011">(Ott, Choi, Cardie, &amp; Hancock, 2011)</a> which reported a 90% accuracy using the fake reviews generated through the AMT tool, <a href="#Mukherjee2013b">(Mukherjee, Venkataraman, Liu, &amp; Glance, 2013)</a> experiments showed only a 68% accuracy when they tested Ott’s model on Yelp data. This led the authors to claim that any previous model trained using reviews collected through the AMT tool can only offer near chance accuracy and is useless when applied on real-life data. However, the authors do not rule out the effectiveness of using n-gram features in the model and they proved the largest accuracy obtained on Yelp data was achieved using a combination of behavioral and linguistic features. Their experiments show little improvement over accuracy when adding n-gram features. Probably the most interesting conclusion is that behavioral features considerably outperform n-gram features alone. </p>

<p><a href="#Mukherjee2013">(Mukherjee et al., 2013)</a> built an unsupervised model called the Author Spamicity Model that aims to split the users into two clusters - truthful users and spammers. The intuition is that the two types of users are naturally separable due to the behavioral footprints left behind when writing reviews. The authors studied the distributional divergence between the two types and tested their model on real-life Amazon reviews. Most of the behavioral features in the model have been previously used in two previous studies by <a href="#Mukherjee2012">(Mukherjee, Liu, &amp; Glance, 2012)</a> and <a href="#Mukherjee2013b">(Mukherjee, Venkataraman, Liu, &amp; Glance, 2013)</a>. In these studies though, the model was trained using supervised learning. The novelty about the proposed method in this paper is a posterior density analysis of each of the features used. This analysis is meant to validate the relevance of each model feature and also increase the knowledge on their expected values for truthful and fake reviews respectively.</p>

<p><a href="#Fei2013">(Fei et al., 2013)</a> focused on detecting spammers that write reviews in short bursts. They represented the reviewers and the relationships between them in a graph and used a graph propagation method to classify reviewers as spammers. Classification was done using supervised learning, by employing human evaluation of the identified honest/deceptive reviewers. The authors relied on behavioral features to detect periods in time when review bursts per product coincided with reviewer burst, i.e. a reviewer is very prolific just as when a number of reviews which is higher than the usual average of reviews for a particular product is recorded. The authors discarded singleton reviewers from the initial dataset, since these provide little behavior information - all the model features used in the burst detection model require extensive reviewing history for each user. By discarding singleton reviewers, this method is similar to the one proposed by <a href="#Mukherjee2012">(Mukherjee, Liu, &amp; Glance, 2012)</a>. These methods can thus only detect fake reviews written by elite users on a review platform. Exploiting review posting bursts is an intuitive way to obtain smaller time windows where suspicious activity occurs. This can be seen as a way to break the fake review detection method into smaller chunks and employ other methods which have to work with considerably less data points. This would decrease the computational and time complexity of the detection algorithm. </p>

<p><a href="#Mukherjee2013a">(Mukherjee, Venkataraman, Liu, &amp; Glance, 2013)</a> made an interesting observation in their study: the spammers caught by Yelp’s filter seem to have “overdone faking” in their try to sound more genuine. In their deceptive reviews, they tried to use words that appear in genuine reviews almost equally frequently, thus avoiding to reuse the exact same words in their reviews. This is exactly the reason why a cosine similarity measure is not enough to catch subtle spammers in real life scenarios, such as Yelp’s. </p>

<h1 id="references">References</h1>
<ol class="bibliography"><li><span id="Jindal2008">Jindal, N., &amp; Liu, B. (2008). Opinion Spam and Analysis. In <i>Proceedings of the 2008 International Conference on Web Search and
	Data Mining</i> (pp. 219–230). New York, NY, USA: ACM. doi:10.1145/1341531.1341560</span></li>
<li><span id="Lim2010">Lim, E.-P., Nguyen, V.-A., Jindal, N., Liu, B., &amp; Lauw, H. W. (2010). Detecting Product Review Spammers Using Rating Behaviors. In <i>Proceedings of the 19<sup>th</sup> ACM International Conference
	on Information and Knowledge Management</i> (pp. 939–948). New York, NY, USA: ACM. doi:10.1145/1871437.1871557</span></li>
<li><span id="Ott2011">Ott, M., Choi, Y., Cardie, C., &amp; Hancock, J. T. (2011). Finding Deceptive Opinion Spam by Any Stretch of the Imagination. In <i>Proceedings of the 49<sup>th</sup> Annual Meeting of the Association
	for Computational Linguistics: Human Language Technologies - Volume
	1</i> (pp. 309–319). Stroudsburg, PA, USA: Association for Computational Linguistics. Retrieved from http://dl.acm.org/citation.cfm?id=2002472.2002512</span></li>
<li><span id="Mukherjee2013">Mukherjee, A., Kumar, A., Liu, B., Wang, J., Hsu, M., Castellanos, M., &amp; Ghosh, R. (2013). Spotting Opinion Spammers Using Behavioral Footprints. In <i>Proceedings of the 19<sup>th</sup> ACM SIGKDD International
	Conference on Knowledge Discovery and Data Mining</i> (pp. 632–640). New York, NY, USA: ACM. doi:10.1145/2487575.2487580</span></li>
<li><span id="Mukherjee2012">Mukherjee, A., Liu, B., &amp; Glance, N. (2012). Spotting Fake Reviewer Groups in Consumer Reviews. In <i>Proceedings of the 21<sup>st</sup> International Conference
	on World Wide Web</i> (pp. 191–200). New York, NY, USA: ACM. doi:10.1145/2187836.2187863</span></li>
<li><span id="Mukherjee2013a">Mukherjee, A., Venkataraman, V., Liu, B., &amp; Glance, N. (2013). What yelp fake review filter might be doing. In <i>Proceedings of the International Conference on Weblogs and Social
	Media</i>.</span></li>
<li><span id="Mukherjee2013b">Mukherjee, A., Venkataraman, V., Liu, B., &amp; Glance, N. (2013). <i>Fake Review Detection: Classification and Analysis of Real and Pseudo
	Reviews</i>. UIC-CS-03-2013. Technical Report.</span></li>
<li><span id="Wang2012">Wang, G., Xie, S., Liu, B., &amp; Yu, P. S. (2012). Identify Online Store Review Spammers via Social Review Graph. <i>ACM Trans. Intell. Syst. Technol.</i>, <i>3</i>(4), 61:1–61:21. doi:10.1145/2337542.2337546</span></li>
<li><span id="Xie2012">Xie, S., Wang, G., Lin, S., &amp; Yu, P. S. (2012). Review Spam Detection via Time Series Pattern Discovery. In <i>Proceedings of the 21<sup>st</sup> International Conference
	Companion on World Wide Web</i> (pp. 635–636). New York, NY, USA: ACM. doi:10.1145/2187980.2188164</span></li>
<li><span id="Feng2012">Feng, S., Xing, L., Gogar, A., &amp; Choi, Y. (2012). Distributional Footprints of Deceptive Product Reviews. In J. G. Breslin, N. B. Ellison, J. G. Shanahan, &amp; Z. Tufekci (Eds.), <i>ICWSM</i>. The AAAI Press. Retrieved from http://dblp.uni-trier.de/db/conf/icwsm/icwsm2012.html#FengXGC12</span></li>
<li><span id="Li2011">Li, F., Huang, M., Yang, Y., &amp; Zhu, X. (2011). Learning to Identify Review Spam. In <i>Proceedings of the Twenty-Second International Joint Conference on
	Artificial Intelligence - Volume Volume Three</i> (pp. 2488–2493). AAAI Press. doi:10.5591/978-1-57735-516-8/IJCAI11-414</span></li>
<li><span id="Fei2013">Fei, G., Mukherjee, A., Liu, B., Hsu, M., Castellanos, M., &amp; Ghosh, R. (2013). Exploiting Burstiness in Reviews for Review Spammer Detection. In <i>Seventh International AAAI Conference on Weblogs and Social Media</i>.</span></li></ol>
